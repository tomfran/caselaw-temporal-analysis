
- tolto stopwords: abbiamo anche provato qualche modello lda e lsi e aggiunto stopwords
// 
attento che tengono conto della distribuzione delle parole, facciamolo in maniera raffinata, usiamo la doc frequency e inverse doc, e usare soglie su quelle, non eliminarle categoricamente

- lemmatizzazione: lentissima, per ora non la facciamo, sostituiamo con stemming? ha consigli?

// 
- tokenizzazione frasi con nltk
- spacy frase per frase, è sbagliato passargli i token, passiamogli la frase, più veloce e migliore qualitativamente

//guardarsi spacy

// prendiamo le frasi, una frase alla volta a spacy, lemma, tfif, idf per capire le stopwords

// in generale una stopword lo è se IDF lo dice, altrimenti non a priori

Topic modelling: 
- come guidare? Peso nei vector, vector delle sole parole di interesse (wikipedia?) e clustering guidato?

// semisupervised topic modelling, lda forzato, partendo da insiemi di parole di partenza, 

in generale, lavoro di 

2) per LDA dobbiamo usare un count vectorizer o va bene tfidf?
3) lemmatization è lentissima con spacy // 6 ore, pensavamo di usare stemming o qualche altro lemmatizer come alternativa.
4) provando topic modelling compaiono sempre le stesse parole, le aggiungiamo alle stopwords?
5) come "guidare" LDA e clustering? per ora abbiamo provato a modificare i vettori aumentando il peso di alcune parole.

1) geograficamente scegliamo le 4 corti presenti in illinois o estendiamo agli altri dataset?