8/11:

- prove con modello md di spacy: 
    serve per i vector delle parole, impiega 30 secondi su 100 documenti;
    https://spacy.io/models/en#en_core_web_md
    Provato con la pipeline senza parser ma con senter, non cambia nulla
    https://spacy.io/usage/spacy-101#pipelines.

- vectorizer su testi gia tokenizzati:
    https://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/
    riscritta la classe vectorizer, tolte quelle astratte

- tokenizer che salva su disco e carica

Considerazioni:
- non è possibile tokenizzare in locale con il modello md, usa 30gb di ram
- su google colab: 

    modello md,     process -1,     niente batch size,  100 doc,    67s 
    modello md,     process -1,     batch size 50,      100 doc,    68s
    modello md,     process 1,      batch size 50,      100 doc,    19s
    modello md,     process 1,      batch size 1000,    10k doc,    19s


    n.b. serve impostare spawn sennò crasha tutto sul multiprocessing, 
    probabilmente è lento quello.

- si possono utilizzare le intersezioni di query per creare l'andamento delle coppie


17/11
- tante parole di interesse non sono presenti nei token filtrati (rimosse cose che non compaiono più dell'1 percento)
- rifare tokenizzazione tenendo tutto? da testare su magari 100 documenti e vedere se si trova di più

18/11
- tantissime parole appaiono in meno dell'1 percento dei documenti, è fattibile poichè 
il numero di casi legati ad esempio alla droga potrebbero essere molto meno di altri
- riducendo i token a quelli che appaiono almeno 20 volte si trovano quasi tutti i 
termini di interesse definiti nella consegna
- lda classico non porta a nulla di interessante, si deve testare quello guided

21/11
Calcolare coherence score del modello lda per capire quanti topic servono, 
fare grid search per alpha e beta, prima di fare guided lda.

