8/11:

- prove con modello md di spacy: 
    serve per i vector delle parole, impiega 30 secondi su 100 documenti;
    https://spacy.io/models/en#en_core_web_md
    Provato con la pipeline senza parser ma con senter, non cambia nulla
    https://spacy.io/usage/spacy-101#pipelines.

- vectorizer su testi gia tokenizzati:
    https://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/
    riscritta la classe vectorizer, tolte quelle astratte

- tokenizer che salva su disco e carica

Considerazioni:
- non è possibile tokenizzare in locale con il modello md, usa 30gb di ram
- su google colab: 

    modello md,     process -1,     niente batch size,  100 doc,    67s 
    modello md,     process -1,     batch size 50,      100 doc,    68s
    modello md,     process 1,      batch size 50,      100 doc,    19s
    modello md,     process 1,      batch size 1000,    10k doc,    19s


    n.b. serve impostare spawn sennò crasha tutto sul multiprocessing, 
    probabilmente è lento quello.

- si possono utilizzare le intersezioni di query per creare l'andamento delle coppie
