RICERCA FATTA
1. Guidare topic modelling: Incorporating Lexical Priors into Topic Models
2. Query expansion: expansion con word embeddings (trova paper)
3. Semantic shift dei termini negli anni: Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change

PREPROCESSING
Notebook 01-02

1. Rimosso i campi inutili, unito le opinions, eliminati numeri e simboli.
2. Diviso il dataset in annate.
3. Tokenizzazione con Spacy (en_md), tenuti i lemmi dei 'NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV'.
4. Considerati solo i documenti della corte d'Appello (120k).

STUDIO TOKEN
Notebook 03

1. Studio frequenza token.
2. Ricerca delle parole di interesse.

ESPANSIONE PAROLE DI INTERESSE
Notebook 04

1. Usato GoogleNews word embeddings per espandere le parole di interesse.
2. Per ogni parola di interesse prendere le 100 parole più simili. 
3. Separate le parole composte in parole singole.
4. Unione delle parole individuate da GoogleNews che dei loro lemmi. 
5. Filtraggio parole non presenti nel corpus.
6. Pulizia manuale dei token finali.

TOPIC MODELLING:
Notebook 05

1. Caricamento token da disco, solo della corte d'Appello (motivazione, che arriva un pò di tutto).
2. Filtering dei token mantenendo le parole di interesse, rimuovendo:
    - parole più corte di 3 caratteri, 
    - termini che compaiono in oltre il 50% dei documenti e meno di 20 volte.
3. Ottimizzazione di un LDA classico, fatto su Colab, testati da 5 a 32 topic con Halving Grid Search
4. Aggiunta al dataset informazione sul topic del documento.
5. Andamento topic nel tempo: visualizzo distribuzione anni per ogni topic.
6. Differenze tra distribuzione dei topic nelle corti.
7. Distribuzione parole per topic.

WORD EMBEDDINGS
Notebook 07

1. Allenato modello sulla corte d'appello, visualizzate le parole di interesse in 2d.
2. Allenati modelli sugli intervalli di 5 anni.
3. Prendendo spunto da "histwords", allineati i modelli per renderli confrontabili, individuando semantic shifts dei termini, da perfezionare.