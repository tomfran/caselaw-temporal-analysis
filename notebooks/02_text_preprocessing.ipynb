{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worldwide-reader",
   "metadata": {},
   "source": [
    "# Text tokenization and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dental-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from src.dataset import Dataset\n",
    "from src.parallel_vectorizer_tokenizer import BatchTokenizer, FastTfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-rubber",
   "metadata": {},
   "source": [
    "## Load processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imported-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(dataset_path=\"../data/raw/data.jsonl\", save_path=\"../data/processed/processed.jsonl\")\n",
    "data = dataset.load_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-reliance",
   "metadata": {},
   "source": [
    "## Create list of texts to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "upper-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list = [document[\"opinions\"][i][\"text\"] for document in data for i in range(len(document[\"opinions\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-gibraltar",
   "metadata": {},
   "source": [
    "## Vectorize text list with Spacy tokenizer\n",
    "\n",
    "We then save the vectors and vectorizer to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "invalid-cardiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cannabis'], ['weed'], ['cocaine'], ['methamphetamine'], ['drugs'], ['marijuana'], ['mdma'], ['lsd'], ['ketamina'], ['heroin'], ['fentanyl'], ['narcotics'], ['weapons'], ['gun'], ['knife'], ['weapon'], ['firearm'], ['rifle'], ['carabine'], ['shotgun'], ['assaults'], ['sword'], ['blunt'], ['investigation'], ['gang'], ['mafia'], ['serial'], ['killer'], ['rape'], ['thefts'], ['recidivism'], ['arrest'], ['ethnicity'], ['caucasian'], ['afroamerican'], ['hispanic'], ['robbery'], ['cybercrime']]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'weed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-75a8838788e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFastTfIdfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincreaseWeightImportantWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Universita\\Magistrale\\secondo semestre\\information retrieval\\project\\legal-texts-information-retrieval\\src\\parallel_vectorizer_tokenizer.py\u001b[0m in \u001b[0;36mincreaseWeightImportantWords\u001b[1;34m(self, vectors, multiplier)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimportantWord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreProcessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtopicWords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimportant_topics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopicWords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mposition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimportantWord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m             \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'weed'"
     ]
    }
   ],
   "source": [
    "vec = FastTfIdfVectorizer(texts_list[:20], BatchTokenizer())\n",
    "vectors = vec.vectors()\n",
    "vec.increaseWeightImportantWords(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1db096-a8cf-49c2-aa2b-92ba27d85bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.save_vectors_vectorizer(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6c9fd-80d8-4cd3-9973-06fdc42c5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-breach",
   "metadata": {},
   "source": [
    "## Load data from npy and pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectors, loaded_vec = FastTfIdfVectorizer.load_vectors_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(loaded_vectors.toarray(), columns=loaded_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loaded_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297041ce-70e2-4a53-b5a6-4b1d97a4c67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
