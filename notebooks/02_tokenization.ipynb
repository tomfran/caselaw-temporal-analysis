{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worldwide-reader",
   "metadata": {},
   "source": [
    "# Text tokenization and vectorization\n",
    "We now use Spacy to tokenize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dental-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.dataset import Dataset\n",
    "from src.tokenizers import BatchTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-rubber",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "Dataset is loaded partition by partition, tokenized and saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imported-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(dataset_path=\"../data/raw/data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b2d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [1760, 1800, 1820, 1840, 1860, 1880, \n",
    "         1900, 1920, 1940, 1960, 1980, 2000]\n",
    "\n",
    "tokens_folder = \"../data/processed/tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6373fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = BatchTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19dddef",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "For each epoch, the text is loaded, tokenized and saved to disk.\n",
    "The tokenizer use Spacy pipe method to speed up the computation. This cell is really intensive therefore it is executed on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e7b8e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 1760: 1 documents\n",
      "Processing year 1800: 5 documents\n",
      "Processing year 1820: 440 documents\n",
      "Processing year 1840: 2657 documents\n",
      "Processing year 1860: 9255 documents\n",
      "Processing year 1880: 19648 documents\n",
      "Processing year 1900: 28932 documents\n",
      "Processing year 1920: 26954 documents\n",
      "Processing year 1940: 14668 documents\n",
      "Processing year 1960: 33683 documents\n",
      "Processing year 1980: 35641 documents\n",
      "Processing year 2000: 11262 documents\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    \n",
    "    texts = [el[\"text\"] for el in dataset.load_dataset(year)]\n",
    "    print(f\"Processing year {year}: {len(texts)} documents\")\n",
    "    tokens = bt.tokenize(texts)\n",
    "    \n",
    "    with open(f\"{tokens_folder}/{year}.json\", \"w\") as f:\n",
    "        f.write(json.dumps(tokens))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
