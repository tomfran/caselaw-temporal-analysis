{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as lda\n",
    "\n",
    "from src.seeds import Seeds\n",
    "from src.dataset import Dataset\n",
    "from src.vectorizers import TokenVectorizer\n",
    "from src.lda_utils import get_word_relevance, get_words_relevance, print_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fran/.local/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 1.0.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "d = Dataset()\n",
    "seeds = Seeds()\n",
    "vectors, vectorizer = TokenVectorizer.load_vectors_vectorizer(method=\"count\")\n",
    "lda_model = pickle.load(open(\"../data/models/IAC_exp_seed_minf_10_max_50%.pk\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "narcotics, weapons, investigation = seeds.get_final_filtered_seeds()\n",
    "total = narcotics.union(weapons).union(investigation)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v, idx) for idx, v in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 23.728,\n",
       " 1: 2.574,\n",
       " 2: 10.009,\n",
       " 3: 2.878,\n",
       " 4: 8.32,\n",
       " 5: 7.289,\n",
       " 6: 2.607,\n",
       " 7: 4.442,\n",
       " 8: 4.776,\n",
       " 9: 2.579,\n",
       " 10: 4.522,\n",
       " 11: 2.394,\n",
       " 12: 3.563,\n",
       " 13: 20.318}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_relevance = get_words_relevance(total, word2id, vocab, lda_model, normalize=True)\n",
    "topic_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 23.728), (13, 20.318), (2, 10.009)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_topics = sorted(topic_relevance.items(), key=lambda x : -x[1])[:2]\n",
    "relevant_set = set([e[0] for e in relevant_topics])\n",
    "relevant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3992\n",
      "15949\n",
      "27928\n",
      "29446\n",
      "32530\n",
      "33529\n",
      "33723\n",
      "33800\n",
      "33817\n",
      "33818\n",
      "33819\n"
     ]
    }
   ],
   "source": [
    "final_tokens = []\n",
    "\n",
    "for year in [2000, 1980, 1960, 1940, 1920, 1900, 1860, 1840, 1820, 1800, 1760]:\n",
    "    \n",
    "    tokens = d.load_dataset(year=year, fields={\"tokens\", \"topic\"})    \n",
    "    for t in tokens:\n",
    "        if np.argmax(t[\"topic\"]) in relevant_set:\n",
    "            final_tokens.append(t[\"tokens\"])\n",
    "    \n",
    "    print(len(final_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = defaultdict(lambda:0)\n",
    "for doc in final_tokens:\n",
    "    # for w in doc:\n",
    "    for w in set(doc):        \n",
    "        freq[w] += 1\n",
    "        \n",
    "def sel_criterium(w):\n",
    "    return (w in total) or ((len(w) >= 3) and (10 < freq[w] < 0.5*len(final_tokens)))\n",
    "    \n",
    "final_tokens = [[w for w in doc if sel_criterium(w)] for doc in final_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 24308\n"
     ]
    }
   ],
   "source": [
    "dv = TokenVectorizer(final_tokens, method=\"count\")\n",
    "\n",
    "vectors = dv.vectors()\n",
    "print(f\"Vocabulary length: {len(dv.vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_jobs=-1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lda(n_components=10, n_jobs=-1)\n",
    "model.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0\n",
      "40912.49*car + 21766.07*arrest + 20837.02*man + 19230.94*gun + 15370.24*burglary + 14982.06*doubt + 14606.74*door + 14345.15*store + 14184.14*prove + 12312.06*steal\n",
      "\n",
      "Topic: 1\n",
      "34749.64*prosecutor + 34004.88*juror + 28016.52*judge + 18362.78*comment + 17562.72*examination + 16307.33*instruction + 15871.13*objection + 15772.78*prejudice + 15449.59*fair + 15012.11*death\n",
      "\n",
      "Topic: 2\n",
      "25642.1*test + 18899.16*section + 13760.37*alcohol + 11216.99*drive + 10294.83*blood + 9474.21*statute + 8979.82*code + 8687.84*vehicle + 7806.35*ilcs + 7537.04*west\n",
      "\n",
      "Topic: 3\n",
      "75725.05*murder + 36874.05*victim + 31550.56*instruction + 29636.91*death + 22217.76*gun + 20077.75*armed + 20054.15*shoot + 19710.06*attempt + 18768.55*kill + 17897.51*degree\n",
      "\n",
      "Topic: 4\n",
      "44955.21*search + 28883.82*warrant + 21025.18*arrest + 19860.63*drug + 19056.78*substance + 17467.61*cocaine + 17246.13*possession + 15230.22*control + 12130.41*information + 12027.32*united\n",
      "\n",
      "Topic: 5\n",
      "60657.94*petition + 45097.35*file + 42030.06*attorney + 32155.38*judge + 29096.97*proceeding + 23863.83*post + 23760.46*claim + 21512.1*plea + 21415.92*petitioner + 21129.62*dismiss\n",
      "\n",
      "Topic: 6\n",
      "59714.89*section + 44449.71*statute + 35024.04*probation + 33445.44*plea + 29231.44*impose + 27665.68*code + 25470.1*par + 20697.09*sentencing + 19130.2*felony + 18864.13*count\n",
      "\n",
      "Topic: 7\n",
      "50584.3*plaintiff + 22625.09*confession + 20602.61*attorney + 15605.82*room + 14569.24*home + 14535.86*come + 13990.33*mrs + 12578.42*admit + 12441.75*house + 11412.51*examination\n",
      "\n",
      "Topic: 8\n",
      "45147.68*victim + 28037.35*sexual + 27723.09*identification + 27208.6*man + 22633.33*identify + 22320.31*robbery + 20032.46*complainant + 19105.84*apartment + 18029.77*home + 16424.41*assault\n",
      "\n",
      "Topic: 9\n",
      "43139.45*car + 28715.19*arrest + 23822.25*stop + 23779.05*vehicle + 14240.91*drive + 11893.83*suppress + 10575.26*driver + 9748.03*gang + 9494.6*probable + 8056.18*station\n"
     ]
    }
   ],
   "source": [
    "print_topics(model, \n",
    "             dv.vectorizer, \n",
    "             n_top_words=10, \n",
    "             only_interesting=False, \n",
    "             interesting_set=total)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
