{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "The goal is to find three topics from the collection, the firt one regarding drugs, the second weapons, and the third investigation. \n",
    "\n",
    "We start with a classic model to then test the guided lda approach. We do not expect the first one to find the three topics we want, while the second should guide the topic modelling towards the required goal.\n",
    "\n",
    "References:\n",
    "https://medium.com/analytics-vidhya/how-i-tackled-a-real-world-problem-with-guidedlda-55ee803a6f0d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# from gensim.models import LsiModel, LdaMulticore\n",
    "# from gensim.test.utils import common_dictionary, common_corpus\n",
    "from sklearn.decomposition import LatentDirichletAllocation as lda\n",
    "\n",
    "\n",
    "from lda import guidedlda as glda\n",
    "\n",
    "# import guidedlda\n",
    "\n",
    "# import pyLDAvis.sklearn\n",
    "# pyLDAvis.enable_notebook()\n",
    "\n",
    "from src.dataset import Dataset\n",
    "from src.vectorizers import TokenVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "# load only the year specified\n",
    "tokens = dataset.load_dataset(year=1820, tokens=True)\n",
    "# load all years\n",
    "# tokens = dataset.load_dataset(tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtra i dati, se vuoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[w for w in doc if len(w) >= 3] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the documents\n",
    "The vectorized is a tfidf one, we use the output to fit the lda model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 7462\n"
     ]
    }
   ],
   "source": [
    "dv = TokenVectorizer(tokens, method=\"count\")\n",
    "\n",
    "vectors = dv.vectors()\n",
    "dv.save_vectors_vectorizer(vectors)\n",
    "print(f\"Vocabulary length: {len(dv.vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, vectorizer = TokenVectorizer.load_vectors_vectorizer(method=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic LDA model\n",
    "\n",
    "The number of topics is set to three, while alpha and beta have values proposed in the literature. \n",
    "\n",
    "Griffiths TL, Steyvers M (2004). “Finding Scientific Topics.” Proceedings of the National Academy of Sciences of the United States of America, 101, 5228–5235."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTopics = 4\n",
    "# alpha = 50/numTopics\n",
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "\n",
    "lda_model = lda(n_components = numTopics, \n",
    "                doc_topic_prior= alpha, \n",
    "                topic_word_prior = beta, \n",
    "                random_state=0, \n",
    "                n_jobs=-1)\n",
    "\n",
    "lda_output = lda_model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics relevant words\n",
    "\n",
    "The next step is to check the words for each topic, results are interesting and expected, bu twe can't see a distinction between the topics we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0\n",
      "power, law, act, state, court, office, constitution, case, may, officer\n",
      "\n",
      "Topic: 1\n",
      "court, judgment, case, justice, circuit, defendant, note, error, cause, plaintiff\n",
      "\n",
      "Topic: 2\n",
      "court, judgment, defendant, plaintiff, say, case, make, law, plea, act\n",
      "\n",
      "Topic: 3\n",
      "court, defendant, plaintiff, action, case, note, evidence, judgment, give, may\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "topic_words = {}\n",
    "for topic, comp in enumerate(lda_model.components_): \n",
    "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "    topic_words[topic] = [vocab[i] for i in word_idx]\n",
    "    \n",
    "for topic, words in topic_words.items():\n",
    "    print('\\nTopic: %d' % topic)\n",
    "    print('%s' % ', '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider only words of interest\n",
    "We now print the word distribution, considering only interesting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "narcotics = ['cannabis', 'cocaine', 'methamphetamine', 'drugs', 'drug', 'marijuana', \n",
    "             'ecstasy', 'lsd', 'ketamine', 'heroin', 'fentanyl', 'overdose']\n",
    "\n",
    "weapons = ['gun', 'knife', 'weapon', 'firearm', 'rifle', 'carabine', 'shotgun', 'handgun', \n",
    "           'revolver', 'musket', 'pistol', 'derringer', 'assault', 'rifle', 'sword', 'blunt']\n",
    "\n",
    "investigation = ['gang', 'mafia', 'serial',  'killer', 'rape', 'theft', 'recidivism', \n",
    "                 'arrest', 'robbery', 'cybercrime', 'cyber', 'crime']\n",
    "\n",
    "interesting_set = set(narcotics + weapons + investigation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0\n",
      "crime, arrest, theft, assault, robbery\n",
      "\n",
      "Topic: 1\n",
      "arrest, crime, assault, robbery, theft\n",
      "\n",
      "Topic: 2\n",
      "arrest, assault, crime, robbery, theft\n",
      "\n",
      "Topic: 3\n",
      "assault, crime, arrest, robbery, theft\n"
     ]
    }
   ],
   "source": [
    "topic_words = {}\n",
    "for topic, comp in enumerate(lda_model.components_): \n",
    "    word_idx = np.argsort(comp)[::-1]\n",
    "    topic_words[topic] = [w for w in [vocab[i] for i in word_idx] if w in interesting_set][:n_top_words]\n",
    "    \n",
    "for topic, words in topic_words.items():\n",
    "    print('\\nTopic: %d' % topic)\n",
    "    print('%s' % ', '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the topics blends together even considering only the words of interest, LDA must be guided. \n",
    "\n",
    "## Guided LDA approach\n",
    "We now guide the lda process by setting some seeds, exploiting the model defined by the GuidedLDA package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = dict((v, idx) for idx, v in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannabis not found in vocabulary\n",
      "cocaine not found in vocabulary\n",
      "methamphetamine not found in vocabulary\n",
      "drugs not found in vocabulary\n",
      "drug not found in vocabulary\n",
      "marijuana not found in vocabulary\n",
      "ecstasy not found in vocabulary\n",
      "lsd not found in vocabulary\n",
      "ketamine not found in vocabulary\n",
      "heroin not found in vocabulary\n",
      "fentanyl not found in vocabulary\n",
      "overdose not found in vocabulary\n",
      "gang not found in vocabulary\n",
      "mafia not found in vocabulary\n",
      "serial not found in vocabulary\n",
      "killer not found in vocabulary\n",
      "rape not found in vocabulary\n",
      "recidivism not found in vocabulary\n",
      "cybercrime not found in vocabulary\n",
      "cyber not found in vocabulary\n",
      "gun not found in vocabulary\n",
      "knife not found in vocabulary\n",
      "weapon not found in vocabulary\n",
      "firearm not found in vocabulary\n",
      "rifle not found in vocabulary\n",
      "carabine not found in vocabulary\n",
      "shotgun not found in vocabulary\n",
      "handgun not found in vocabulary\n",
      "revolver not found in vocabulary\n",
      "musket not found in vocabulary\n",
      "pistol not found in vocabulary\n",
      "derringer not found in vocabulary\n",
      "rifle not found in vocabulary\n",
      "sword not found in vocabulary\n",
      "blunt not found in vocabulary\n"
     ]
    }
   ],
   "source": [
    "seed_topic_list = [narcotics, investigation, weapons]\n",
    "seed_topics = {}\n",
    "\n",
    "for i, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        if word in word2id:\n",
    "            seed_topics[word2id[word]] = i\n",
    "        else:\n",
    "            print(f\"{word} not found in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 440\n",
      "INFO:lda:vocab_size: 7462\n",
      "INFO:lda:n_words: 171104\n",
      "INFO:lda:n_topics: 3\n",
      "INFO:lda:n_iter: 5\n",
      "INFO:lda:<0> log likelihood: -1448480\n",
      "INFO:lda:<4> log likelihood: -1376458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.guidedlda.GuidedLDA at 0x7f301fff42e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glda_model = glda.GuidedLDA(n_topics=3, \n",
    "                       n_iter=5, \n",
    "                       random_state=0, \n",
    "                       refresh=10, \n",
    "                       alpha=alpha, \n",
    "                       eta=beta)\n",
    "\n",
    "glda_model.fit(vectors, \n",
    "          seed_topics=seed_topics, \n",
    "          seed_confidence=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "robbery arrest assault crime theft\n",
      "Topic 1:\n",
      "arrest crime assault robbery theft\n",
      "Topic 2:\n",
      "assault theft crime arrest robbery\n"
     ]
    }
   ],
   "source": [
    "topic_word = glda_model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][::-1]\n",
    "    topic_words = [w for w in topic_words if w in interesting_set][:n_top_words]\n",
    "    print(f\"Topic {i}:\\n{' '.join(topic_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocabulary = { dv.vectorizer.vocabulary_[k]:k for k in dv.vectorizer.vocabulary_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LsiModel(vectors.transpose(), id2word=reverse_vocabulary, num_topics=numTopics) \n",
    "topics = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topWords = []\n",
    "for topicno in range(numTopics):\n",
    "    print('Topic {}'.format(topicno))\n",
    "    print([(x, round(y, 2)) for x, y in model.show_topic(topicno, topn=10)], '\\n')\n",
    "    topWords.append([(x) for x, y in model.show_topic(topicno, topn=10)])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
