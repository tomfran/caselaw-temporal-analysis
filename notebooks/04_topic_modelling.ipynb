{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "We start with a classic model to then test the guided lda approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as lda\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "# import pyLDAvis.sklearn\n",
    "# pyLDAvis.enable_notebook()\n",
    "\n",
    "from src.dataset import Dataset\n",
    "from src.vectorizers import TokenTfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(dataset_path=\"\", save_path=f\"../data/processed/tokenized_processed.json\")\n",
    "tokens = dataset.load_text_list(field_name=\"tokens\", size=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the documents\n",
    "The vectorized is a tfidf one, we use the output to fit the lda model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 71597\n"
     ]
    }
   ],
   "source": [
    "dv = TokenTfidfVectorizer(tokens)\n",
    "\n",
    "vectors = dv.vectors()\n",
    "dv.save_vectors_vectorizer(vectors)\n",
    "print(f\"Vocabulary length: {len(dv.vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTopics = 3\n",
    "alpha = 50/numTopics\n",
    "beta = 0.1\n",
    "\n",
    "lda_model = lda(n_components = numTopics, \n",
    "                doc_topic_prior= alpha, \n",
    "                topic_word_prior = beta, \n",
    "                random_state=0)\n",
    "\n",
    "lda_output = lda_model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "  court, defendant, plaintiff, illinois, state, evidence, judgment, trial, opinion, case\n",
      "Topic: 1\n",
      "  court, defendant, illinois, plaintiff, trial, case, appellant, justice, evidence, mr.\n",
      "Topic: 2\n",
      "  defendant, court, plaintiff, illinois, opinion, mr., case, trial, deliver, make\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "vocab = dv.vectorizer.get_feature_names()\n",
    "topic_words = {}\n",
    "for topic, comp in enumerate(lda_model.components_): \n",
    "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "    topic_words[topic] = [vocab[i] for i in word_idx]\n",
    "    \n",
    "for topic, words in topic_words.items():\n",
    "    print('Topic: %d' % topic)\n",
    "    print('  %s' % ', '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocabulary = { dv.vectorizer.vocabulary_[k]:k for k in dv.vectorizer.vocabulary_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LsiModel(vectors.transpose(), id2word=reverse_vocabulary, num_topics=numTopics) \n",
    "topics = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "[('defendant', 0.33), ('court', 0.28), ('plaintiff', 0.21), ('illinois', 0.18), ('trial', 0.14), ('case', 0.12), ('evidence', 0.11), ('state', 0.11), ('judgment', 0.1), ('appellant', 0.1), ('say', 0.1), ('n.e.2d', 0.09), ('jury', 0.09), ('make', 0.09), ('people', 0.08), ('would', 0.08), ('motion', 0.08), ('act', 0.08), ('such', 0.08), ('error', 0.08), ('app', 0.08), ('appellee', 0.07), ('order', 0.07), ('section', 0.07), ('may', 0.07), ('time', 0.07), ('property', 0.07), ('file', 0.07), ('3d', 0.07), ('contract', 0.07)] \n",
      "\n",
      "Topic 1\n",
      "[('mr.', 0.41), ('justice', 0.38), ('opinion', 0.38), ('presiding', 0.36), ('deliver', 0.32), ('court', 0.27), ('publish', 0.26), ('full', 0.16), ('defendant', -0.13), ('mcsurely', 0.12), ('oâ€™connor', 0.11), ('barnes', 0.11), ('matchett', 0.1), ('gridley', 0.09), ('scanlan', 0.06), ('friend', 0.06), ('plaintiff', -0.05), ('thomson', 0.04), ('illinois', -0.04), ('taylor', 0.04), ('wilson', 0.04), ('n.e.2d', -0.04), ('trial', -0.04), ('sullivan', 0.04), ('holdom', 0.04), ('people', -0.03), ('burke', 0.03), ('wolfe', 0.03), ('state', -0.03), ('dove', 0.03)] \n",
      "\n",
      "Topic 2\n",
      "[('defendant', -0.48), ('appellant', 0.24), ('appellee', 0.19), ('trial', -0.16), ('people', -0.15), ('sentence', -0.14), ('n.e.2d', -0.13), ('contract', 0.12), ('say', 0.12), ('property', 0.12), ('curiam', 0.11), ('decree', 0.11), ('pay', 0.1), ('land', 0.1), ('deed', 0.1), ('bill', 0.1), ('conviction', -0.09), ('offense', -0.09), ('police', -0.09), ('3d', -0.08), ('company', 0.08), ('such', 0.08), ('officer', -0.08), ('estate', 0.08), ('claimant', 0.08), ('sale', 0.07), ('guilty', -0.07), ('judgment', 0.07), ('city', 0.07), ('counsel', -0.07)] \n",
      "\n",
      "{'n.e.2d', 'defendant', 'people', 'trial'}\n"
     ]
    }
   ],
   "source": [
    "topWords = []\n",
    "for topicno in range(numTopics):\n",
    "    print('Topic {}'.format(topicno))\n",
    "    print([(x, round(y, 2)) for x, y in model.show_topic(topicno, topn=30)], '\\n')\n",
    "    topWords.append([(x) for x, y in model.show_topic(topicno, topn=30)])\n",
    "    \n",
    "print(set.intersection(*map(set,topWords)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
