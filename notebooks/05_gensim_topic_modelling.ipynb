{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling with Gensim\n",
    "\n",
    "In this notebook Gensim is used to find topics from the documents, we first study coherence measures to find the number of topics in the collection, then we visualize results.\n",
    "\n",
    "The basic idea is to study the differences between topics in different times. So we first present the methodology followed given an era, and then perform the study on all eras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "\n",
    "# utils\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# topic modelling\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaMulticore, LdaModel\n",
    "\n",
    "# visualization\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the tokenized documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = json.load(open(\"../data/processed/tokens/1820.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating required structures\n",
    "The topic modelling requires this three structures to work, the first one gives a mapping from an id to a word, \n",
    "second one is the tokenized collection, and the third one is a list for each document of word id, frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(tokens)\n",
    "texts = tokens\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(f\"Corpus[0]: {corpus[0][:5]}...\")\n",
    "print(f\"id2word[0]: {id2word[0]}\")\n",
    "print(f\"Corpus[0] readable: {[(id2word[cp[0]], cp[1]) for cp in corpus[0][:5]]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting LDA models and computing coherence measures\n",
    "\n",
    "Lda is unsupervised, a crucial parameter is the number of topics to find in the collection. To avoid choosing at random one could compute the coherence measure given a certain number of topics and pick the highest coherence.\n",
    "\n",
    "To do so, different models are fitted, and the best one, according to coherence is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_perplexity_values(corpus, id2word, tokens, k=[2,3,4], verbose=True):\n",
    "    lda_models = []\n",
    "    statistics = []\n",
    "    \n",
    "    for topics in k:\n",
    "        if verbose: print(f\"Fitting model with {topics} topics.\")\n",
    "        lda_model = LdaModel(corpus=corpus, \n",
    "                            id2word=id2word, \n",
    "                            num_topics=topics, \n",
    "                            random_state=100, \n",
    "                            update_every=1, \n",
    "                            chunksize=100,\n",
    "                            passes=10, \n",
    "                            alpha='auto', \n",
    "                            per_word_topics=True)\n",
    "        \n",
    "        lda_models.append(lda_model)\n",
    "        coherence_model = CoherenceModel(model=lda_model, texts=tokens, \n",
    "                                             dictionary=id2word, coherence='c_v')\n",
    "        \n",
    "        statistics.append({\"topics\" : topics, \n",
    "                          \"coherence\" : coherence_model.get_coherence(),\n",
    "                          \"log_perplexity\" : lda_model.log_perplexity(corpus)})\n",
    "        \n",
    "    return lda_models, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = range(2, 7)\n",
    "lda_models, statistics = compute_coherence_perplexity_values(corpus, id2word, tokens, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting coherence measures\n",
    "The best model is the one with four topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k, [x[\"coherence\"] for x in statistics])\n",
    "plt.title(\"Lda models coherence score\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best, ind = max(zip(statistics, range(len(statistics))), key=lambda x:x[0][\"coherence\"], x[1])\n",
    "print(f\"Best model found:\\n{best}\")\n",
    "best_model = lda_models[ind]\n",
    "print(\"\\nBest model topics:\")\n",
    "best_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyLDAvis visualization\n",
    "\n",
    "This tool offers a graphical visualization of the topics, a good visualization has big topics that are far from each others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(best_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together\n",
    "\n",
    "After presenting the methodology, we run different searches on each era to find the best number of topics, the result is a model for each era that achieved the bet coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dir = \"../data/processed/tokens\"\n",
    "epochs_files = [f\"{tokens_dir}/{f}\" for f in sorted(os.listdir(tokens_dir))]\n",
    "\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "\n",
    "k = range(min_topics, max_topics+1)\n",
    "\n",
    "best_models = {}\n",
    "for epoch in epochs_files:\n",
    "    # extracting year name\n",
    "    name = int(epoch.split(\"/\")[-1].split(\".\")[0])\n",
    "    print(f\"Computing best model for {name}:\")\n",
    "    \n",
    "    # creating required structures\n",
    "    texts = json.load(open(epoch, \"r\"))\n",
    "    id2word = corpora.Dictionary(texts)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # fitting models and picking the best one\n",
    "    lda_models, statistics = compute_coherence_perplexity_values(corpus, id2word, texts, k, verbose=False)\n",
    "    best, ind = max(zip(statistics, range(len(statistics))), key=lambda x:x[0][\"coherence\"])\n",
    "    print(f\"\\tBest one: {best}\\n\")\n",
    "    best_models[name] = lda_models[ind]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
