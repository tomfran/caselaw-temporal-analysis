{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from functools import reduce\n",
    "\n",
    "from src.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 2000\n",
      "Training one year models:\n",
      "2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 Training 10 years models:\n",
      "2000 2010 \n",
      "\n",
      "Processing: 1980\n",
      "Training one year models:\n",
      "1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 Training 10 years models:\n",
      "1980 1990 \n",
      "\n",
      "Processing: 1960\n",
      "Training one year models:\n",
      "1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 Training 10 years models:\n",
      "1960 1970 \n",
      "\n",
      "Processing: 1940\n",
      "Training one year models:\n",
      "1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 Training 10 years models:\n",
      "1940 1950 \n",
      "\n",
      "Processing: 1920\n",
      "Training one year models:\n",
      "1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 Training 10 years models:\n",
      "1920 1930 \n",
      "\n",
      "Processing: 1900\n",
      "Training one year models:\n",
      "1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 Training 10 years models:\n",
      "1900 1910 \n",
      "\n",
      "Processing: 1860\n",
      "Training one year models:\n",
      "1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 Training 10 years models:\n",
      "1860 1870 \n",
      "\n",
      "Processing: 1840\n",
      "Training one year models:\n",
      "1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 Training 10 years models:\n",
      "1840 1850 \n",
      "\n",
      "Processing: 1820\n",
      "Training one year models:\n",
      "1820 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 Training 10 years models:\n",
      "1820 1830 \n",
      "\n",
      "Processing: 1800\n",
      "Training one year models:\n",
      "1819 Training 10 years models:\n",
      "1810 \n",
      "\n",
      "Processing: 1760\n",
      "Training one year models:\n",
      "1771 Training 10 years models:\n",
      "1770 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "interval = 10\n",
    "vector_size = 300\n",
    "dataset = Dataset()\n",
    "\n",
    "one_year_path = \"../data/models/one_year_time_vectors\"\n",
    "five_year_path = \"../data/models/five_year_time_vectors\"\n",
    "\n",
    "\n",
    "for year in [2000, 1980, 1960, 1940, 1920, 1900, 1860, 1840, 1820, 1800, 1760]:\n",
    "    print(\"Processing:\", year)\n",
    "    tokens = dataset.load_dataset(year=year, fields={\"tokens\", \"decision_date\"})\n",
    "\n",
    "    years_tokens = defaultdict(lambda:[])\n",
    "    epochs_tokens = defaultdict(lambda:[])\n",
    "    for t in tokens:\n",
    "        # merging 2011 and 2010, improved vocabulary when aligning\n",
    "        d1 = min(t[\"decision_date\"], 2010)\n",
    "        years_tokens[d1].append(t[\"tokens\"])\n",
    "        # adding this to train 10 years models\n",
    "        d2 = d1 - d1%interval\n",
    "        epochs_tokens[d2].append(t[\"tokens\"])\n",
    "    \n",
    "    \n",
    "    print(\"Training one year models:\")\n",
    "    for date, date_tokens in years_tokens.items():\n",
    "        print(date, end=\" \")\n",
    "        model = Word2Vec(date_tokens, min_count=5, workers=4, vector_size = vector_size)\n",
    "        model.save(f\"{one_year_path}/{date}_w2v.model\")\n",
    "    \n",
    "    print(\"Training 10 years models:\")\n",
    "    for date, date_tokens in epochs_tokens.items():\n",
    "        print(date, end=\" \")\n",
    "        model = Word2Vec(date_tokens, min_count=5, workers=4, vector_size = vector_size)\n",
    "        model.save(f\"{five_year_path}/{date}_w2v.model\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
