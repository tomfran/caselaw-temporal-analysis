{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "The goal of this notebook is to run topic modelling on the dataset. For performance reasons we limit ourselves to the Illinois Appellate Court containing approximately 120k documents.\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/#9buildldamodelwithsklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation as lda\n",
    "from lda import guidedlda as glda\n",
    "\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "from src.dataset import Dataset\n",
    "from src.vectorizers import TokenVectorizer\n",
    "from src.lda_utils import get_word_relevance, get_words_relevance, print_topics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens loading and preprocessing\n",
    "\n",
    "The first step is to load the tokenized documents and to filter the tokens, removing those that are over and under used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "tokens = dataset.load_dataset(year=None, \n",
    "                              tokens=True, \n",
    "                              courts={\"Illinois Appellate Court\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123915"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "We count the occurrences of each token and filter those that do not match the criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = defaultdict(lambda:0)\n",
    "for doc in tokens:\n",
    "    # for w in doc:\n",
    "    for w in set(doc):        \n",
    "        freq[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "narcotics = ['cannabis', 'cocaine', 'methamphetamine', 'drugs', 'drug', 'marijuana', \n",
    "             'ecstasy', 'lsd', 'ketamine', 'heroin', 'fentanyl', 'overdose']\n",
    "\n",
    "weapons = ['gun', 'knife', 'weapon', 'firearm', 'rifle', 'carabine', 'shotgun', 'handgun', \n",
    "           'revolver', 'musket', 'pistol', 'derringer', 'assault', 'rifle', 'sword', 'blunt']\n",
    "\n",
    "investigation = ['gang', 'mafia', 'serial',  'killer', 'rape', 'theft', 'recidivism', \n",
    "                 'arrest', 'robbery', 'cybercrime', 'cyber', 'crime']\n",
    "\n",
    "interesting_set = set(narcotics + weapons + investigation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_criterium(w):\n",
    "    return (w in interesting_set) or (len(w) >= 3) and (15 < freq[w] < 0.5*len(tokens))\n",
    "    \n",
    "tokens = [[w for w in doc if sel_criterium(w)] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "The next step is to vectorize the data with a count vectorizer. The results are saved on disk to avi having to run the first cells each time, saving RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 38855\n"
     ]
    }
   ],
   "source": [
    "dv = TokenVectorizer(tokens, method=\"count\")\n",
    "\n",
    "vectors = dv.vectors()\n",
    "dv.save_vectors_vectorizer(vectors)\n",
    "print(f\"Vocabulary length: {len(dv.vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, vectorizer = TokenVectorizer.load_vectors_vectorizer(method=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic LDA model\n",
    "The first model used is the sklearn implementation of LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTopics = 3\n",
    "# alpha = 50/numTopics\n",
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "\n",
    "lda_model = lda(n_components = numTopics, \n",
    "                doc_topic_prior= alpha, \n",
    "                topic_word_prior = beta, \n",
    "                random_state=0, \n",
    "                n_jobs=-1)\n",
    "\n",
    "lda_output = lda_model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing topics words\n",
    "Here are the found most relevant words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topics(lda_model, \n",
    "             vectorizer, \n",
    "             n_top_words=5, \n",
    "             only_interesting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering only words of interest\n",
    "Here are only the words of interest in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topics(lda_model, \n",
    "             vectorizer, \n",
    "             n_top_words=5, \n",
    "             only_interesting=True, \n",
    "             interesting_set=interesting_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most relevant topics given a word, and a list of words\n",
    "We define two functions that computes the relevance of each topic given a word and a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v, idx) for idx, v in enumerate(vocab))\n",
    "\n",
    "get_word_relevance(weapons[0], word2id, vocab, lda_model, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [weapons, narcotics, investigation]:\n",
    "    print(get_words_relevance(l, word2id, vocab, lda_model, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal number of topics\n",
    "\n",
    "Before proceeding with other models, we find the optimal number of topics with a grid search approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(vectors))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(vectors))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    'n_components'  : [8, 10, 12]\n",
    "}\n",
    "\n",
    "fun = lda()\n",
    "\n",
    "model = GridSearchCV(fun, param_grid=search_params, verbose=1)\n",
    "model.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_model = model.best_estimator_\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, vectors, vectorizer)\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided LDA approach\n",
    "The next part of the notebook is a guided LDA approach. The idea is to set word priors before running lda, to merge interesting words in the same topic. This will lead to a division between the categories of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v, idx) for idx, v in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [narcotics, investigation, weapons]\n",
    "seed_topics = {}\n",
    "\n",
    "for i, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        if word in word2id:\n",
    "            seed_topics[word2id[word]] = i\n",
    "        else:\n",
    "            print(f\"{word} not found in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_numTopics = model.best_params_[\"n_components\"]\n",
    "g_alpha = 0.1\n",
    "g_beta = 0.01\n",
    "g_iter = 100\n",
    "\n",
    "glda_model = glda.GuidedLDA(n_topics=g_numTopics, \n",
    "                            n_iter=g_iter, \n",
    "                            random_state=0, \n",
    "                            refresh=10, \n",
    "                            alpha=g_alpha, \n",
    "                            eta=g_beta)\n",
    "\n",
    "glda_model.fit(vectors, \n",
    "               seed_topics=seed_topics, \n",
    "               seed_confidence=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Guided lda topics\")\n",
    "print_topics(glda_model, \n",
    "             vectorizer, \n",
    "             n_top_words=10, \n",
    "             only_interesting=False)\n",
    "\n",
    "print(\"\\nTopics with only interesting words\")\n",
    "print_topics(glda_model, \n",
    "             vectorizer, \n",
    "             n_top_words=10, \n",
    "             only_interesting=True, \n",
    "             interesting_set=interesting_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [weapons, narcotics, investigation]:\n",
    "    print(get_words_relevance(l, word2id, vocab, glda_model, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pyLDAvis.sklearn.prepare(glda_model, vectors, vectorizer)\n",
    "panel"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
