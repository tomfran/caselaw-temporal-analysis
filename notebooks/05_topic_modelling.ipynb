{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "The goal is to find three topics from the collection, the firt one regarding drugs, the second weapons, and the third investigation. \n",
    "\n",
    "We start with a classic model to then test the guided lda approach. We do not expect the first one to find the three topics we want, while the second should guide the topic modelling towards the required goal.\n",
    "\n",
    "References:\n",
    "https://medium.com/analytics-vidhya/how-i-tackled-a-real-world-problem-with-guidedlda-55ee803a6f0d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as lda\n",
    "from lda import guidedlda as glda\n",
    "\n",
    "# import pyLDAvis.sklearn\n",
    "# pyLDAvis.enable_notebook()\n",
    "\n",
    "from src.dataset import Dataset\n",
    "from src.vectorizers import TokenVectorizer\n",
    "from src.lda_utils import get_word_relevance, get_words_relevance, print_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "# load only the year specified\n",
    "# year = None # carica tutto\n",
    "year = None # carico solo quel ventennio \n",
    "\n",
    "tokens = dataset.load_dataset(year=1960, \n",
    "                              tokens=True, \n",
    "                              courts={\"Illinois Supreme Court\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4891"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = defaultdict(lambda:0)\n",
    "for doc in tokens:\n",
    "    for w in doc:\n",
    "    # for w in set(doc):        \n",
    "        freq[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "narcotics = ['cannabis', 'cocaine', 'methamphetamine', 'drugs', 'drug', 'marijuana', \n",
    "             'ecstasy', 'lsd', 'ketamine', 'heroin', 'fentanyl', 'overdose']\n",
    "\n",
    "weapons = ['gun', 'knife', 'weapon', 'firearm', 'rifle', 'carabine', 'shotgun', 'handgun', \n",
    "           'revolver', 'musket', 'pistol', 'derringer', 'assault', 'rifle', 'sword', 'blunt']\n",
    "\n",
    "investigation = ['gang', 'mafia', 'serial',  'killer', 'rape', 'theft', 'recidivism', \n",
    "                 'arrest', 'robbery', 'cybercrime', 'cyber', 'crime']\n",
    "\n",
    "interesting_set = set(narcotics + weapons + investigation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_criterium(w):\n",
    "    return (w in interesting_set) or (len(w) >= 3) and (10 < freq[w] < 0.5*len(tokens))\n",
    "    \n",
    "tokens = [[w for w in doc if sel_criterium(w)] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the documents\n",
    "The vectorized is a tfidf one, we use the output to fit the lda model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 12000\n"
     ]
    }
   ],
   "source": [
    "dv = TokenVectorizer(tokens, method=\"count\")\n",
    "\n",
    "vectors = dv.vectors()\n",
    "dv.save_vectors_vectorizer(vectors)\n",
    "print(f\"Vocabulary length: {len(dv.vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading precomputed vectors, this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, vectorizer = TokenVectorizer.load_vectors_vectorizer(method=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic LDA model\n",
    "\n",
    "The number of topics is set to three, while alpha and beta have values proposed in the literature. \n",
    "\n",
    "Griffiths TL, Steyvers M (2004). “Finding Scientific Topics.” Proceedings of the National Academy of Sciences of the United States of America, 101, 5228–5235."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTopics = 10\n",
    "# alpha = 50/numTopics\n",
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "\n",
    "lda_model = lda(n_components = numTopics, \n",
    "                doc_topic_prior= alpha, \n",
    "                topic_word_prior = beta, \n",
    "                random_state=0, \n",
    "                n_jobs=-1)\n",
    "\n",
    "lda_output = lda_model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics relevant words\n",
    "\n",
    "The next step is to check the words for each topic, results are interesting and expected, bu twe can't see a distinction between the topics we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0\n",
      "676.74*safety + 662.33*driver + 652.65*teacher + 617.18*negligence + 612.1*care + 603.65*education + 567.83*truck + 561.69*standard + 547.29*operate + 534.6*church\n",
      "\n",
      "Topic: 1\n",
      "1618.18*deed + 1185.82*title + 945.12*grand + 868.01*contempt + 785.54*probate + 744.28*trustee + 656.57*testator + 541.41*decree + 507.08*executor + 503.53*document\n",
      "\n",
      "Topic: 2\n",
      "4950.84*arrest + 3712.68*crime + 2574.77*robbery + 1985.23*identification + 1956.09*guilt + 1835.01*gun + 1673.84*station + 1557.41*suppress + 1459.09*room + 1410.36*door\n",
      "\n",
      "Topic: 3\n",
      "1723.44*zone + 1534.1*lot + 1262.89*park + 1035.94*north + 1027.17*road + 1014.97*avenue + 991.85*tract + 937.69*south + 879.07*east + 852.05*propose\n",
      "\n",
      "Topic: 4\n",
      "916.26*insure + 888.98*automobile + 773.14*coverage + 737.14*vehicle + 595.17*insurer + 463.89*clause + 421.56*check + 393.01*uninsured + 360.32*book + 350.14*life\n",
      "\n",
      "Topic: 5\n",
      "1046.38*decree + 985.11*wife + 947.48*mother + 842.82*divorce + 796.36*joint + 784.78*husband + 768.71*sign + 767.75*minor + 751.4*loan + 751.18*custody\n",
      "\n",
      "Topic: 6\n",
      "2011.09*appoint + 1945.76*election + 1372.66*probation + 1303.23*crime + 1299.91*plead + 1189.49*discharge + 1165.65*allegation + 1151.83*information + 1123.85*affidavit + 1117.45*delay\n",
      "\n",
      "Topic: 7\n",
      "1169.99*negligence + 1043.71*limitation + 997.13*common + 919.49*tort + 827.45*committee + 817.85*recovery + 807.8*suit + 798.57*protection + 750.75*association + 739.73*civil\n",
      "\n",
      "Topic: 8\n",
      "1718.01*arbitrator + 1342.6*disability + 1319.52*workman + 1234.35*doctor + 1188.4*manifest + 1147.09*week + 1141.97*pain + 967.51*permanent + 946.38*loss + 942.45*leg\n",
      "\n",
      "Topic: 9\n",
      "2172.07*assessment + 1886.13*rate + 1771.61*administrative + 1390.5*director + 1337.75*bond + 1329.82*class + 1310.72*license + 1308.93*lease + 1261.88*assess + 1258.43*occupation\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model, \n",
    "             vectorizer, \n",
    "             n_top_words=10, \n",
    "             only_interesting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider only words of interest\n",
    "We now print the word distribution, considering only interesting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0\n",
      "55.11*drug + 4.06*assault + 3.21*crime + 0.94*arrest + 0.01*pistol + 0.01*cocaine + 0.01*derringer + 0.01*overdose + 0.01*recidivism + 0.01*weapon\n",
      "\n",
      "Topic: 1\n",
      "13.02*crime + 2.89*sword + 1.84*arrest + 0.97*theft + 0.39*assault + 0.08*weapon + 0.04*knife + 0.01*handgun + 0.01*gun + 0.01*gang\n",
      "\n",
      "Topic: 2\n",
      "4950.84*arrest + 3712.68*crime + 2574.77*robbery + 1835.01*gun + 992.87*rape + 812.89*weapon + 661.04*assault + 588.91*knife + 487.17*drug + 428.6*heroin\n",
      "\n",
      "Topic: 3\n",
      "19.17*drug + 2.49*lsd + 1.6*theft + 1.08*serial + 0.01*blunt + 0.01*assault + 0.01*gang + 0.01*firearm + 0.01*arrest + 0.01*crime\n",
      "\n",
      "Topic: 4\n",
      "17.88*assault + 11.65*theft + 1.87*crime + 1.61*shotgun + 0.4*drug + 0.29*rape + 0.01*arrest + 0.01*gang + 0.01*firearm + 0.01*blunt\n",
      "\n",
      "Topic: 5\n",
      "42.53*lsd + 22.35*drug + 3.91*blunt + 1.97*crime + 0.33*theft + 0.3*killer + 0.02*cocaine + 0.01*assault + 0.01*marijuana + 0.01*sword\n",
      "\n",
      "Topic: 6\n",
      "1303.23*crime + 758.41*arrest + 522.52*drug + 481.25*robbery + 340.68*theft + 202.71*marijuana + 188.87*rape + 144.44*assault + 141.57*cannabis + 55.42*heroin\n",
      "\n",
      "Topic: 7\n",
      "50.64*drug + 10.26*assault + 6.82*gang + 3.08*crime + 2.89*overdose + 0.45*cannabis + 0.01*marijuana + 0.01*arrest + 0.01*derringer + 0.01*blunt\n",
      "\n",
      "Topic: 8\n",
      "39.59*drug + 3.47*gang + 3.25*killer + 0.13*overdose + 0.01*crime + 0.01*assault + 0.01*arrest + 0.01*firearm + 0.01*derringer + 0.01*cannabis\n",
      "\n",
      "Topic: 9\n",
      "12.15*drug + 4.92*firearm + 3.54*serial + 3.52*theft + 2.21*sword + 0.01*crime + 0.01*blunt + 0.01*assault + 0.01*arrest + 0.01*derringer\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_model, \n",
    "             vectorizer, \n",
    "             n_top_words=10, \n",
    "             only_interesting=True, \n",
    "             interesting_set=interesting_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most relevant topics given a word, and a list of words\n",
    "\n",
    "We no test how much the interesting words get merged together in topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v, idx) for idx, v in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.001,\n",
       " 1: 0.001,\n",
       " 2: 99.995,\n",
       " 3: 0.001,\n",
       " 4: 0.001,\n",
       " 5: 0.001,\n",
       " 6: 0.001,\n",
       " 7: 0.001,\n",
       " 8: 0.001,\n",
       " 9: 0.001}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_relevance(weapons[0], word2id, vocab, lda_model, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.075, 1: 0.062, 2: 96.064, 3: 0.003, 4: 0.35, 5: 0.072, 6: 3.057, 7: 0.185, 8: 0.002, 9: 0.129}\n",
      "{0: 2.375, 1: 0.003, 2: 48.801, 3: 0.935, 4: 0.02, 5: 2.795, 6: 40.508, 7: 2.325, 8: 1.712, 9: 0.526}\n",
      "{0: 0.027, 1: 0.1, 2: 80.274, 3: 0.017, 4: 0.087, 5: 0.017, 6: 19.328, 7: 0.063, 8: 0.043, 9: 0.045}\n"
     ]
    }
   ],
   "source": [
    "for l in [weapons, narcotics, investigation]:\n",
    "    print(get_words_relevance(l, word2id, vocab, lda_model, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result implies that narcotics are present in the topic number 2, 6, weapons get the topic number 2, while investigation the 2 and the 6.\n",
    "\n",
    "We would want three different topics for each interesting list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal number of topics\n",
    "\n",
    "We now run the lda model with a variable number of topics, we compute coherence to decide the best number of topics.\n",
    "\n",
    "The model is made for gensim, but the documentation states:\n",
    "\n",
    "*This function also supports models from lda and sklearn (by passing topic_word_distrib, dtm and vocab)!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "\n",
    "# metric_coherence_gensim(measure='c_v', \n",
    "#                         top_n=25, \n",
    "#                         topic_word_distrib=lda_model.components_, \n",
    "#                         dtm=vectors, \n",
    "#                         vocab=np.array(vectorizer.get_feature_names()), \n",
    "#                         texts=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the topics blends together even considering only the words of interest, LDA must be guided. \n",
    "\n",
    "## Guided LDA approach\n",
    "We now guide the lda process by setting some seeds, exploiting the model defined by the GuidedLDA package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v, idx) for idx, v in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugs not found in vocabulary\n",
      "ecstasy not found in vocabulary\n",
      "ketamine not found in vocabulary\n",
      "fentanyl not found in vocabulary\n",
      "mafia not found in vocabulary\n",
      "cybercrime not found in vocabulary\n",
      "cyber not found in vocabulary\n",
      "carabine not found in vocabulary\n",
      "musket not found in vocabulary\n"
     ]
    }
   ],
   "source": [
    "seed_topic_list = [narcotics, investigation, weapons]\n",
    "seed_topics = {}\n",
    "\n",
    "for i, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        if word in word2id:\n",
    "            seed_topics[word2id[word]] = i\n",
    "        else:\n",
    "            print(f\"{word} not found in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 4891\n",
      "INFO:lda:vocab_size: 12000\n",
      "INFO:lda:n_words: 1986290\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 100\n",
      "INFO:lda:<0> log likelihood: -21616291\n",
      "INFO:lda:<10> log likelihood: -17651783\n",
      "INFO:lda:<20> log likelihood: -17087954\n",
      "INFO:lda:<30> log likelihood: -16932813\n",
      "INFO:lda:<40> log likelihood: -16860177\n",
      "INFO:lda:<50> log likelihood: -16812843\n",
      "INFO:lda:<60> log likelihood: -16780606\n",
      "INFO:lda:<70> log likelihood: -16758106\n",
      "INFO:lda:<80> log likelihood: -16741971\n",
      "INFO:lda:<90> log likelihood: -16726448\n",
      "INFO:lda:<99> log likelihood: -16713549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.guidedlda.GuidedLDA at 0x7f852d965310>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_numTopics = 10\n",
    "g_alpha = 0.1\n",
    "g_beta = 0.01\n",
    "g_iter = 100\n",
    "\n",
    "glda_model = glda.GuidedLDA(n_topics=g_numTopics, \n",
    "                            n_iter=g_iter, \n",
    "                            random_state=0, \n",
    "                            refresh=10, \n",
    "                            alpha=g_alpha, \n",
    "                            eta=g_beta)\n",
    "\n",
    "glda_model.fit(vectors, \n",
    "               seed_topics=seed_topics, \n",
    "               seed_confidence=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided lda topics\n",
      "\n",
      "Topic: 0\n",
      "0.01*information + 0.01*drug + 0.01*agent + 0.01*informer + 0.01*client + 0.01*test + 0.01*contempt + 0.0*standard + 0.0*material + 0.0*mental\n",
      "\n",
      "Topic: 1\n",
      "0.02*arrest + 0.01*crime + 0.01*accuse + 0.01*robbery + 0.01*convict + 0.0*penitentiary + 0.0*waive + 0.0*appoint + 0.0*prosecutor + 0.0*prejudice\n",
      "\n",
      "Topic: 2\n",
      "0.01*identification + 0.01*gun + 0.01*station + 0.01*door + 0.01*drive + 0.01*crime + 0.01*room + 0.0*tavern + 0.0*observe + 0.0*guilt\n",
      "\n",
      "Topic: 3\n",
      "0.01*election + 0.01*class + 0.01*protection + 0.01*local + 0.01*administrative + 0.01*limitation + 0.01*government + 0.0*adopt + 0.0*classification + 0.0*license\n",
      "\n",
      "Topic: 4\n",
      "0.01*arbitrator + 0.01*disability + 0.01*doctor + 0.01*workman + 0.01*manifest + 0.01*week + 0.01*pain + 0.01*loss + 0.01*permanent + 0.01*leg\n",
      "\n",
      "Topic: 5\n",
      "0.02*zone + 0.01*lot + 0.01*park + 0.01*road + 0.01*north + 0.01*tract + 0.01*south + 0.01*avenue + 0.01*east + 0.01*west\n",
      "\n",
      "Topic: 6\n",
      "0.01*negligence + 0.01*vehicle + 0.01*automobile + 0.01*recovery + 0.01*product + 0.01*contractor + 0.01*tort + 0.01*common + 0.01*injure + 0.0*motor\n",
      "\n",
      "Topic: 7\n",
      "0.01*rate + 0.01*railroad + 0.01*water + 0.01*utility + 0.01*commerce + 0.01*system + 0.01*gas + 0.01*facility + 0.01*operation + 0.01*fair\n",
      "\n",
      "Topic: 8\n",
      "0.01*decree + 0.01*wife + 0.01*joint + 0.01*husband + 0.01*share + 0.01*divorce + 0.01*trustee + 0.01*probate + 0.01*decedent + 0.01*account\n",
      "\n",
      "Topic: 9\n",
      "0.01*assessment + 0.01*title + 0.01*deed + 0.01*lease + 0.01*sell + 0.01*occupation + 0.01*loan + 0.01*assess + 0.01*taxpayer + 0.01*levy\n",
      "\n",
      "Topics with only interesting words\n",
      "\n",
      "Topic: 0\n",
      "0.01*drug + 0.0*heroin + 0.0*marijuana + 0.0*cannabis + 0.0*serial + 0.0*lsd + 0.0*cocaine + 0.0*methamphetamine + 0.0*overdose + 0.0*crime\n",
      "\n",
      "Topic: 1\n",
      "0.02*arrest + 0.01*crime + 0.01*robbery + 0.0*theft + 0.0*rape + 0.0*assault + 0.0*weapon + 0.0*drug + 0.0*pistol + 0.0*knife\n",
      "\n",
      "Topic: 2\n",
      "0.01*gun + 0.01*crime + 0.0*robbery + 0.0*rape + 0.0*weapon + 0.0*knife + 0.0*assault + 0.0*shotgun + 0.0*revolver + 0.0*pistol\n",
      "\n",
      "Topic: 3\n",
      "0.0*firearm + 0.0*sword + 0.0*gun + 0.0*handgun + 0.0*gang + 0.0*blunt + 0.0*cannabis + 0.0*assault + 0.0*arrest + 0.0*derringer\n",
      "\n",
      "Topic: 4\n",
      "0.0*drug + 0.0*blunt + 0.0*killer + 0.0*recidivism + 0.0*heroin + 0.0*gun + 0.0*gang + 0.0*handgun + 0.0*firearm + 0.0*cocaine\n",
      "\n",
      "Topic: 5\n",
      "0.0*drug + 0.0*heroin + 0.0*firearm + 0.0*gang + 0.0*handgun + 0.0*gun + 0.0*blunt + 0.0*cannabis + 0.0*assault + 0.0*arrest\n",
      "\n",
      "Topic: 6\n",
      "0.0*theft + 0.0*heroin + 0.0*firearm + 0.0*gang + 0.0*handgun + 0.0*gun + 0.0*blunt + 0.0*cannabis + 0.0*assault + 0.0*arrest\n",
      "\n",
      "Topic: 7\n",
      "0.0*drug + 0.0*derringer + 0.0*gang + 0.0*gun + 0.0*handgun + 0.0*firearm + 0.0*assault + 0.0*arrest + 0.0*blunt + 0.0*weapon\n",
      "\n",
      "Topic: 8\n",
      "0.0*handgun + 0.0*gun + 0.0*heroin + 0.0*firearm + 0.0*gang + 0.0*blunt + 0.0*cannabis + 0.0*assault + 0.0*arrest + 0.0*derringer\n",
      "\n",
      "Topic: 9\n",
      "0.0*gang + 0.0*heroin + 0.0*handgun + 0.0*firearm + 0.0*gun + 0.0*blunt + 0.0*cannabis + 0.0*assault + 0.0*arrest + 0.0*derringer\n"
     ]
    }
   ],
   "source": [
    "print(\"Guided lda topics\")\n",
    "print_topics(glda_model, \n",
    "             vectorizer, \n",
    "             n_top_words=10, \n",
    "             only_interesting=False)\n",
    "\n",
    "print(\"\\nTopics with only interesting words\")\n",
    "print_topics(glda_model, \n",
    "             vectorizer, \n",
    "             n_top_words=10, \n",
    "             only_interesting=True, \n",
    "             interesting_set=interesting_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.004, 1: 7.245, 2: 92.069, 3: 0.547, 4: 0.111, 5: 0.006, 6: 0.004, 7: 0.005, 8: 0.005, 9: 0.003}\n",
      "{0: 96.542, 1: 1.355, 2: 0.002, 3: 0.002, 4: 1.013, 5: 1.07, 6: 0.004, 7: 0.005, 8: 0.004, 9: 0.003}\n",
      "{0: 0.666, 1: 71.659, 2: 27.348, 3: 0.001, 4: 0.062, 5: 0.002, 6: 0.259, 7: 0.001, 8: 0.001, 9: 0.001}\n"
     ]
    }
   ],
   "source": [
    "for l in [weapons, narcotics, investigation]:\n",
    "    print(get_words_relevance(l, word2id, vocab, glda_model, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall partition is better, narcotics get the topic 0, weapons the number 2, while investigation is at 70 percent on the topic 1 and 30 on the 2."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
